{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        (Intercept)       x1          x2          x3\n",
      "tf_coef    1.022545 1.989774 -0.05028131 0.002042508\n",
      "r_coef     1.015988 1.997655 -0.04860387 0.004851936\n"
     ]
    }
   ],
   "source": [
    "library(tensorflow)\n",
    "# 'X' and 'Y' are placeholders for input data, output data.\n",
    "# We'll fit a regression model using 3 predictors, so our\n",
    "# input matrix 'X' will have three columns.\n",
    "p <- 3L\n",
    "X <- tf$placeholder(\"float\", shape = shape(NULL, p), name = \"x-data\")\n",
    "Y <- tf$placeholder(\"float\", name = \"y-data\")\n",
    "\n",
    "# Define the weights for each column in X. Since we will\n",
    "# have 3 predictors, our 'W' matrix of coefficients will\n",
    "# have 3 elements. We use a 3 x 1 matrix here, rather than\n",
    "# a vector, to ensure TensorFlow understands how to perform\n",
    "# matrix multiplication on 'W' and 'X'.\n",
    "W <- tf$Variable(tf$zeros(list(p, 1L)))\n",
    "b <- tf$Variable(tf$zeros(list(1L)))\n",
    "\n",
    "# Define the model (how estimates of 'Y' are produced)\n",
    "Y_hat <- tf$add(tf$matmul(X, W), b)\n",
    "\n",
    "# Define the cost function.\n",
    "# We seek to minimize the mean-squared error.\n",
    "cost <- tf$reduce_mean(tf$square(Y_hat - Y))\n",
    "\n",
    "# Define the mechanism used to optimize the loss function.\n",
    "# Although normally we'd just use ordinary least squares,\n",
    "# we'll instead use a gradient descent optimizer (since, in\n",
    "# a more typical learning situation, we won't have an easy\n",
    "# mechanism for directly computing the values of coefficients)\n",
    "generator <- tf$train$GradientDescentOptimizer(learning_rate = 0.01)\n",
    "optimizer <- generator$minimize(cost)\n",
    "\n",
    "# Initialize a TensorFlow session for our regression.\n",
    "init <- tf$global_variables_initializer()\n",
    "session <- tf$Session()\n",
    "session$run(init)\n",
    "\n",
    "# Generate some data. The 'true' model will be 'y = 2x + 1';\n",
    "# that is, the 'slope' parameter is '2', and the intercept is '1'.\n",
    "# The variable 'y' will only be associated with the first variable;\n",
    "# the other two variables are just noise.\n",
    "set.seed(123)\n",
    "n <- 250\n",
    "x <- matrix(runif(p * n), nrow = n)\n",
    "y <- matrix(2 * x[, 1] + 1 + (rnorm(n, sd = 0.25)))\n",
    "\n",
    "# Repeatedly run optimizer until the cost is no longer changing.\n",
    "# (We can take advantage of this since we're using gradient descent\n",
    "# as our optimizer)\n",
    "feed_dict <- dict(X = x, Y = y)\n",
    "epsilon <- .Machine$double.eps\n",
    "last_cost <- Inf\n",
    "while (TRUE) {\n",
    "  session$run(optimizer, feed_dict = feed_dict)\n",
    "  current_cost <- session$run(cost, feed_dict = feed_dict)\n",
    "  if (last_cost - current_cost < epsilon) break\n",
    "  last_cost <- current_cost\n",
    "}\n",
    "\n",
    "# Generate an R model and compare coefficients from each fit\n",
    "r_model <- lm(y ~ x)\n",
    "\n",
    "# Collect coefficients from TensorFlow model fit\n",
    "tf_coef <- c(session$run(b), session$run(W))\n",
    "r_coef  <- r_model$coefficients\n",
    "\n",
    "print(rbind(tf_coef, r_coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
