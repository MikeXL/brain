{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiclass classifier evaluation #\n",
    "\n",
    "A bit tongue twist to read, perhaps call it multi label classifier or multi target classifier.. anywhoo, like multi class classifier. Fun\n",
    "\n",
    "Here comes some thoughts on evaluate 'em\n",
    "\n",
    "1. confusion matrix, good old friend\n",
    "2. cross entropy\n",
    "3. micro vs. macro F1\n",
    "4. ROC .. yes, you hear it.. one-vs-rest still function, be cautious though.. and for three class, there is an 3D version of it. 4D for four class, other than tongue twist, brain twist is required\n",
    "5. Lift, Gain?\n",
    "6. Calibration, K-S ?\n",
    "\n",
    "P.S. None of these would not make sense, if not for model comparison purpose, be it comparing to the hypothetical 50-50 draw or an baseline regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reticulate)\n",
    "use_python('/PYTHON_SERVICES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch = import(\"torch\")\n",
    "nn    = import(\"torch.nn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn$Sequential( nn$Linear(4L, 13L), \n",
    "                  nn$ReLU(),\n",
    "                  nn$Linear(13L, 3L),\n",
    "                  nn$Softmax()\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=13, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=13, out_features=3, bias=True)\n",
       "  (3): Softmax()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx <- sample(1:nrow(iris), 30)\n",
    "train <- iris[-idx, ]\n",
    "test <- iris[idx,]\n",
    "# new.data <- matrix(scale(test[,1:4]), nrow=20, ncol=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 4])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train <- torch$from_numpy(as.matrix(iris[-idx, 1:4]))$float()\n",
    "x_val   <- torch$from_numpy(as.matrix(iris[ idx, 1:4]))$float()\n",
    "x_train$size()\n",
    "x_val$size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train <- torch$from_numpy(as.matrix(as.integer(iris[-idx,5])-1))$long()\n",
    "y_train <- torch$squeeze(y_train, 1L)\n",
    "y_val   <- torch$from_numpy(as.matrix(as.integer(iris[idx,5])-1))$long()\n",
    "y_val   <- torch$squeeze(y_val, 1L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train$size()\n",
    "y_val$size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "main <- py_run_string(\"\n",
    "def init_weight(m):\n",
    "    #print m.__class__.__name__\n",
    "    if isinstance(m, nn.Linear):\n",
    "        #nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        #nn.init.constant_(m.bias.data, 0)\n",
    "        m.reset_parameters()\n",
    "        #print m.weight.data, m.bias.data\n",
    "\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=13, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=13, out_features=3, bias=True)\n",
       "  (3): Softmax()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m$apply(main$init_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "EPOCH = 500\n",
    "losses = matrix(0, nrow=EPOCH, ncol=2)\n",
    "\n",
    "optim    <- torch$optim$Adam(m$parameters(), lr=.001)\n",
    "\n",
    "for( e in 1:EPOCH)\n",
    "{\n",
    "        m$train()\n",
    "        optim$zero_grad()\n",
    "        yhat = m(x_train)\n",
    "        # print(yhat)\n",
    "        #l = nn$MSELoss()(yhat, y$float())\n",
    "        l = nn$CrossEntropyLoss()(yhat, y_train)\n",
    "        losses[e, 1] = l$item()\n",
    "        l$backward()\n",
    "        optim$step()\n",
    "\n",
    "        #torch$no_grad() \n",
    "        #torch$set_grad_enabled(F)\n",
    "        m$eval()\n",
    "        pred = m(x_val)\n",
    "        ll = nn$CrossEntropyLoss()(pred, y_val)\n",
    "        losses[e,2] = ll$item()\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       V1               V2        \n",
       " Min.   :0.7479   Min.   :0.8092  \n",
       " 1st Qu.:0.8064   1st Qu.:0.8901  \n",
       " Median :0.8761   Median :0.9896  \n",
       " Mean   :0.9054   Mean   :0.9851  \n",
       " 3rd Qu.:1.0110   3rd Qu.:1.0889  \n",
       " Max.   :1.1294   Max.   :1.1538  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAFoCAMAAAC8KnXeAAAANlBMVEUAAAAA/wBNTU1oaGh8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\nRK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width = 5, repr.plot.height = 3)\n",
    "plot(losses[,1], type=\"l\", col=\"green\", xlab=\"\", ylab=\"\")\n",
    "lines(losses[,2], col=\"grey\", type=\"l\", lty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=13, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=13, out_features=3, bias=True)\n",
       "  (3): Softmax()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m$eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train <- m(x_train)\n",
    "p_val   <- m(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class_train <- torch$max(p_train$data, 1L)\n",
    "pred_class_val   <- torch$max(p_val$data, 1L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   \n",
       "     0  1  2\n",
       "  0 41  0  0\n",
       "  1  0 28  0\n",
       "  2  0  8 43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(pred_class_train[[2]]$numpy(), y_train$numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   \n",
       "     0  1  2\n",
       "  0  9  0  0\n",
       "  1  0 10  0\n",
       "  2  0  4  7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(pred_class_val[[2]]$numpy(), y_val$numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. cross entropy\n",
    "Please see above for training, validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. micro vs. macro F1\n",
    "### 4. ROC \n",
    "### 5. Lift, Gain?\n",
    "### 6. Calibration, K-S ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. dimension reduction\n",
    "Principal components or t-sne, project multi class into two, hence easier to plot them out and colour by label, to see how each group separate from each other.\n",
    "From below, could see both PCA and tsne works well in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_train <- as.data.frame(cbind(p_train$data$numpy(), y_train$data$numpy(), pred_class_train[[2]]$data$numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(result_train) <- c(\"p1\", \"p2\", \"p3\", \"label\", \"pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>p1</th><th scope=col>p2</th><th scope=col>p3</th><th scope=col>label</th><th scope=col>pred</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9421884   </td><td>0.05692599  </td><td>0.0008856279</td><td>0           </td><td>0           </td></tr>\n",
       "\t<tr><td>0.9032198   </td><td>0.09449765  </td><td>0.0022825063</td><td>0           </td><td>0           </td></tr>\n",
       "\t<tr><td>0.9257674   </td><td>0.07277658  </td><td>0.0014560712</td><td>0           </td><td>0           </td></tr>\n",
       "\t<tr><td>0.9025694   </td><td>0.09493569  </td><td>0.0024949568</td><td>0           </td><td>0           </td></tr>\n",
       "\t<tr><td>0.9473773   </td><td>0.05185671  </td><td>0.0007659904</td><td>0           </td><td>0           </td></tr>\n",
       "\t<tr><td>0.9398077   </td><td>0.05920563  </td><td>0.0009866365</td><td>0           </td><td>0           </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " p1 & p2 & p3 & label & pred\\\\\n",
       "\\hline\n",
       "\t 0.9421884    & 0.05692599   & 0.0008856279 & 0            & 0           \\\\\n",
       "\t 0.9032198    & 0.09449765   & 0.0022825063 & 0            & 0           \\\\\n",
       "\t 0.9257674    & 0.07277658   & 0.0014560712 & 0            & 0           \\\\\n",
       "\t 0.9025694    & 0.09493569   & 0.0024949568 & 0            & 0           \\\\\n",
       "\t 0.9473773    & 0.05185671   & 0.0007659904 & 0            & 0           \\\\\n",
       "\t 0.9398077    & 0.05920563   & 0.0009866365 & 0            & 0           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "p1 | p2 | p3 | label | pred | \n",
       "|---|---|---|---|---|---|\n",
       "| 0.9421884    | 0.05692599   | 0.0008856279 | 0            | 0            | \n",
       "| 0.9032198    | 0.09449765   | 0.0022825063 | 0            | 0            | \n",
       "| 0.9257674    | 0.07277658   | 0.0014560712 | 0            | 0            | \n",
       "| 0.9025694    | 0.09493569   | 0.0024949568 | 0            | 0            | \n",
       "| 0.9473773    | 0.05185671   | 0.0007659904 | 0            | 0            | \n",
       "| 0.9398077    | 0.05920563   | 0.0009866365 | 0            | 0            | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  p1        p2         p3           label pred\n",
       "1 0.9421884 0.05692599 0.0008856279 0     0   \n",
       "2 0.9032198 0.09449765 0.0022825063 0     0   \n",
       "3 0.9257674 0.07277658 0.0014560712 0     0   \n",
       "4 0.9025694 0.09493569 0.0024949568 0     0   \n",
       "5 0.9473773 0.05185671 0.0007659904 0     0   \n",
       "6 0.9398077 0.05920563 0.0009866365 0     0   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(result_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained.pca <- prcomp(result_train[, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Importance of components:\n",
       "                          PC1    PC2       PC3\n",
       "Standard deviation     0.5307 0.1831 1.322e-08\n",
       "Proportion of Variance 0.8936 0.1064 0.000e+00\n",
       "Cumulative Proportion  0.8936 1.0000 1.000e+00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(trained.pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add PCA results in\n",
    "result_train <- cbind(result_train, trained.pca$x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>p1</th><th scope=col>p2</th><th scope=col>p3</th><th scope=col>label</th><th scope=col>pred</th><th scope=col>PC1</th><th scope=col>PC2</th><th scope=col>PC3</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9421884    </td><td>0.05692599   </td><td>0.0008856279 </td><td>0            </td><td>0            </td><td>-0.7325754   </td><td>-0.06376726  </td><td> 1.595764e-09</td></tr>\n",
       "\t<tr><td>0.9032198    </td><td>0.09449765   </td><td>0.0022825063 </td><td>0            </td><td>0            </td><td>-0.6934326   </td><td>-0.02635110  </td><td> 1.533778e-08</td></tr>\n",
       "\t<tr><td>0.9257674    </td><td>0.07277658   </td><td>0.0014560712 </td><td>0            </td><td>0            </td><td>-0.7160879   </td><td>-0.04797561  </td><td>-5.209528e-09</td></tr>\n",
       "\t<tr><td>0.9025694    </td><td>0.09493569   </td><td>0.0024949568 </td><td>0            </td><td>0            </td><td>-0.6927077   </td><td>-0.02598426  </td><td> 4.046881e-09</td></tr>\n",
       "\t<tr><td>0.9473773    </td><td>0.05185671   </td><td>0.0007659904 </td><td>0            </td><td>0            </td><td>-0.7377625   </td><td>-0.06883990  </td><td>-1.362159e-08</td></tr>\n",
       "\t<tr><td>0.9398077    </td><td>0.05920563   </td><td>0.0009866365 </td><td>0            </td><td>0            </td><td>-0.7301782   </td><td>-0.06150280  </td><td> 1.602476e-08</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       " p1 & p2 & p3 & label & pred & PC1 & PC2 & PC3\\\\\n",
       "\\hline\n",
       "\t 0.9421884     & 0.05692599    & 0.0008856279  & 0             & 0             & -0.7325754    & -0.06376726   &  1.595764e-09\\\\\n",
       "\t 0.9032198     & 0.09449765    & 0.0022825063  & 0             & 0             & -0.6934326    & -0.02635110   &  1.533778e-08\\\\\n",
       "\t 0.9257674     & 0.07277658    & 0.0014560712  & 0             & 0             & -0.7160879    & -0.04797561   & -5.209528e-09\\\\\n",
       "\t 0.9025694     & 0.09493569    & 0.0024949568  & 0             & 0             & -0.6927077    & -0.02598426   &  4.046881e-09\\\\\n",
       "\t 0.9473773     & 0.05185671    & 0.0007659904  & 0             & 0             & -0.7377625    & -0.06883990   & -1.362159e-08\\\\\n",
       "\t 0.9398077     & 0.05920563    & 0.0009866365  & 0             & 0             & -0.7301782    & -0.06150280   &  1.602476e-08\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "p1 | p2 | p3 | label | pred | PC1 | PC2 | PC3 | \n",
       "|---|---|---|---|---|---|\n",
       "| 0.9421884     | 0.05692599    | 0.0008856279  | 0             | 0             | -0.7325754    | -0.06376726   |  1.595764e-09 | \n",
       "| 0.9032198     | 0.09449765    | 0.0022825063  | 0             | 0             | -0.6934326    | -0.02635110   |  1.533778e-08 | \n",
       "| 0.9257674     | 0.07277658    | 0.0014560712  | 0             | 0             | -0.7160879    | -0.04797561   | -5.209528e-09 | \n",
       "| 0.9025694     | 0.09493569    | 0.0024949568  | 0             | 0             | -0.6927077    | -0.02598426   |  4.046881e-09 | \n",
       "| 0.9473773     | 0.05185671    | 0.0007659904  | 0             | 0             | -0.7377625    | -0.06883990   | -1.362159e-08 | \n",
       "| 0.9398077     | 0.05920563    | 0.0009866365  | 0             | 0             | -0.7301782    | -0.06150280   |  1.602476e-08 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  p1        p2         p3           label pred PC1        PC2        \n",
       "1 0.9421884 0.05692599 0.0008856279 0     0    -0.7325754 -0.06376726\n",
       "2 0.9032198 0.09449765 0.0022825063 0     0    -0.6934326 -0.02635110\n",
       "3 0.9257674 0.07277658 0.0014560712 0     0    -0.7160879 -0.04797561\n",
       "4 0.9025694 0.09493569 0.0024949568 0     0    -0.6927077 -0.02598426\n",
       "5 0.9473773 0.05185671 0.0007659904 0     0    -0.7377625 -0.06883990\n",
       "6 0.9398077 0.05920563 0.0009866365 0     0    -0.7301782 -0.06150280\n",
       "  PC3          \n",
       "1  1.595764e-09\n",
       "2  1.533778e-08\n",
       "3 -5.209528e-09\n",
       "4  4.046881e-09\n",
       "5 -1.362159e-08\n",
       "6  1.602476e-08"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(result_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAANlBMVEUAAAAAzQBNTU1oaGh8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\nOBQMDgWDQ8HgUDA4FAwOBYNDweD8B0I84iU1iy07AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(PC1~PC2, data=result_train, col=(label+1), pch=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(Rtsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained.tsne <- Rtsne(result_train[, 1:3], check_duplicates=F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add tsne result in\n",
    "result_train <- cbind(result_train, trained.tsne$Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames(result_train) <- c(\"p1\", \"p2\", \"p3\", \"label\", \"pred\", \"PC1\", \"PC2\", \"PC3\", \"tsne1\", \"tsne2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>p1</th><th scope=col>p2</th><th scope=col>p3</th><th scope=col>label</th><th scope=col>pred</th><th scope=col>PC1</th><th scope=col>PC2</th><th scope=col>PC3</th><th scope=col>tsne1</th><th scope=col>tsne2</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.9421884    </td><td>0.05692599   </td><td>0.0008856279 </td><td>0            </td><td>0            </td><td>-0.7325754   </td><td>-0.06376726  </td><td> 1.595764e-09</td><td>20.07669     </td><td>-11.86575    </td></tr>\n",
       "\t<tr><td>0.9032198    </td><td>0.09449765   </td><td>0.0022825063 </td><td>0            </td><td>0            </td><td>-0.6934326   </td><td>-0.02635110  </td><td> 1.533778e-08</td><td>17.83134     </td><td>-11.47798    </td></tr>\n",
       "\t<tr><td>0.9257674    </td><td>0.07277658   </td><td>0.0014560712 </td><td>0            </td><td>0            </td><td>-0.7160879   </td><td>-0.04797561  </td><td>-5.209528e-09</td><td>19.10942     </td><td>-11.82753    </td></tr>\n",
       "\t<tr><td>0.9025694    </td><td>0.09493569   </td><td>0.0024949568 </td><td>0            </td><td>0            </td><td>-0.6927077   </td><td>-0.02598426  </td><td> 4.046881e-09</td><td>17.82928     </td><td>-11.43438    </td></tr>\n",
       "\t<tr><td>0.9473773    </td><td>0.05185671   </td><td>0.0007659904 </td><td>0            </td><td>0            </td><td>-0.7377625   </td><td>-0.06883990  </td><td>-1.362159e-08</td><td>20.34388     </td><td>-11.81030    </td></tr>\n",
       "\t<tr><td>0.9398077    </td><td>0.05920563   </td><td>0.0009866365 </td><td>0            </td><td>0            </td><td>-0.7301782   </td><td>-0.06150280  </td><td> 1.602476e-08</td><td>19.93172     </td><td>-11.92846    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllll}\n",
       " p1 & p2 & p3 & label & pred & PC1 & PC2 & PC3 & tsne1 & tsne2\\\\\n",
       "\\hline\n",
       "\t 0.9421884     & 0.05692599    & 0.0008856279  & 0             & 0             & -0.7325754    & -0.06376726   &  1.595764e-09 & 20.07669      & -11.86575    \\\\\n",
       "\t 0.9032198     & 0.09449765    & 0.0022825063  & 0             & 0             & -0.6934326    & -0.02635110   &  1.533778e-08 & 17.83134      & -11.47798    \\\\\n",
       "\t 0.9257674     & 0.07277658    & 0.0014560712  & 0             & 0             & -0.7160879    & -0.04797561   & -5.209528e-09 & 19.10942      & -11.82753    \\\\\n",
       "\t 0.9025694     & 0.09493569    & 0.0024949568  & 0             & 0             & -0.6927077    & -0.02598426   &  4.046881e-09 & 17.82928      & -11.43438    \\\\\n",
       "\t 0.9473773     & 0.05185671    & 0.0007659904  & 0             & 0             & -0.7377625    & -0.06883990   & -1.362159e-08 & 20.34388      & -11.81030    \\\\\n",
       "\t 0.9398077     & 0.05920563    & 0.0009866365  & 0             & 0             & -0.7301782    & -0.06150280   &  1.602476e-08 & 19.93172      & -11.92846    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "p1 | p2 | p3 | label | pred | PC1 | PC2 | PC3 | tsne1 | tsne2 | \n",
       "|---|---|---|---|---|---|\n",
       "| 0.9421884     | 0.05692599    | 0.0008856279  | 0             | 0             | -0.7325754    | -0.06376726   |  1.595764e-09 | 20.07669      | -11.86575     | \n",
       "| 0.9032198     | 0.09449765    | 0.0022825063  | 0             | 0             | -0.6934326    | -0.02635110   |  1.533778e-08 | 17.83134      | -11.47798     | \n",
       "| 0.9257674     | 0.07277658    | 0.0014560712  | 0             | 0             | -0.7160879    | -0.04797561   | -5.209528e-09 | 19.10942      | -11.82753     | \n",
       "| 0.9025694     | 0.09493569    | 0.0024949568  | 0             | 0             | -0.6927077    | -0.02598426   |  4.046881e-09 | 17.82928      | -11.43438     | \n",
       "| 0.9473773     | 0.05185671    | 0.0007659904  | 0             | 0             | -0.7377625    | -0.06883990   | -1.362159e-08 | 20.34388      | -11.81030     | \n",
       "| 0.9398077     | 0.05920563    | 0.0009866365  | 0             | 0             | -0.7301782    | -0.06150280   |  1.602476e-08 | 19.93172      | -11.92846     | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  p1        p2         p3           label pred PC1        PC2        \n",
       "1 0.9421884 0.05692599 0.0008856279 0     0    -0.7325754 -0.06376726\n",
       "2 0.9032198 0.09449765 0.0022825063 0     0    -0.6934326 -0.02635110\n",
       "3 0.9257674 0.07277658 0.0014560712 0     0    -0.7160879 -0.04797561\n",
       "4 0.9025694 0.09493569 0.0024949568 0     0    -0.6927077 -0.02598426\n",
       "5 0.9473773 0.05185671 0.0007659904 0     0    -0.7377625 -0.06883990\n",
       "6 0.9398077 0.05920563 0.0009866365 0     0    -0.7301782 -0.06150280\n",
       "  PC3           tsne1    tsne2    \n",
       "1  1.595764e-09 20.07669 -11.86575\n",
       "2  1.533778e-08 17.83134 -11.47798\n",
       "3 -5.209528e-09 19.10942 -11.82753\n",
       "4  4.046881e-09 17.82928 -11.43438\n",
       "5 -1.362159e-08 20.34388 -11.81030\n",
       "6  1.602476e-08 19.93172 -11.92846"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(result_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeAAAAFoCAMAAAC46dgSAAAANlBMVEUAAAAAzQBNTU1oaGh8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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width = 4, repr.plot.height = 3)\n",
    "plot(tsne1~tsne2, data=result_train,  col=(label+1), pch=label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "?legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
