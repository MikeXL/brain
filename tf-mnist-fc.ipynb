{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“package ‘tensorflow’ was built under R version 3.4.2”"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: loss = 2.20 (0.003 sec)\n",
      "Step 200: loss = 2.01 (0.003 sec)\n",
      "Step 300: loss = 1.82 (0.003 sec)\n",
      "Step 400: loss = 1.49 (0.004 sec)\n",
      "Step 500: loss = 1.18 (0.007 sec)\n",
      "Step 600: loss = 0.95 (0.003 sec)\n",
      "Step 700: loss = 0.68 (0.004 sec)\n",
      "Step 800: loss = 0.69 (0.003 sec)\n",
      "Step 900: loss = 0.52 (0.004 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 47176  Precision @ 1: 0.8577\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4324  Precision @ 1: 0.8648\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 8629  Precision @ 1: 0.8629\n",
      "Step 1000: loss = 0.48 (0.003 sec)\n",
      "Step 1100: loss = 0.50 (0.003 sec)\n",
      "Step 1200: loss = 0.40 (0.003 sec)\n",
      "Step 1300: loss = 0.47 (0.003 sec)\n",
      "Step 1400: loss = 0.41 (0.003 sec)\n",
      "Step 1500: loss = 0.43 (0.004 sec)\n",
      "Step 1600: loss = 0.39 (0.003 sec)\n",
      "Step 1700: loss = 0.43 (0.003 sec)\n",
      "Step 1800: loss = 0.40 (0.003 sec)\n",
      "Step 1900: loss = 0.26 (0.004 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 49109  Precision @ 1: 0.8929\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4503  Precision @ 1: 0.9006\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 8974  Precision @ 1: 0.8974\n",
      "Step 2000: loss = 0.29 (0.005 sec)\n",
      "Step 2100: loss = 0.41 (0.003 sec)\n",
      "Step 2200: loss = 0.43 (0.005 sec)\n",
      "Step 2300: loss = 0.32 (0.003 sec)\n",
      "Step 2400: loss = 0.47 (0.003 sec)\n",
      "Step 2500: loss = 0.50 (0.003 sec)\n",
      "Step 2600: loss = 0.29 (0.003 sec)\n",
      "Step 2700: loss = 0.21 (0.003 sec)\n",
      "Step 2800: loss = 0.33 (0.003 sec)\n",
      "Step 2900: loss = 0.42 (0.004 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 49992  Precision @ 1: 0.9089\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4578  Precision @ 1: 0.9156\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9107  Precision @ 1: 0.9107\n",
      "Step 3000: loss = 0.21 (0.003 sec)\n",
      "Step 3100: loss = 0.23 (0.004 sec)\n",
      "Step 3200: loss = 0.47 (0.003 sec)\n",
      "Step 3300: loss = 0.29 (0.003 sec)\n",
      "Step 3400: loss = 0.36 (0.003 sec)\n",
      "Step 3500: loss = 0.32 (0.004 sec)\n",
      "Step 3600: loss = 0.44 (0.003 sec)\n",
      "Step 3700: loss = 0.26 (0.003 sec)\n",
      "Step 3800: loss = 0.20 (0.003 sec)\n",
      "Step 3900: loss = 0.24 (0.003 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 50527  Precision @ 1: 0.9187\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4624  Precision @ 1: 0.9248\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9187  Precision @ 1: 0.9187\n",
      "Step 4000: loss = 0.33 (0.003 sec)\n",
      "Step 4100: loss = 0.21 (0.003 sec)\n",
      "Step 4200: loss = 0.30 (0.004 sec)\n",
      "Step 4300: loss = 0.25 (0.004 sec)\n",
      "Step 4400: loss = 0.17 (0.003 sec)\n",
      "Step 4500: loss = 0.24 (0.004 sec)\n",
      "Step 4600: loss = 0.46 (0.003 sec)\n",
      "Step 4700: loss = 0.21 (0.003 sec)\n",
      "Step 4800: loss = 0.23 (0.004 sec)\n",
      "Step 4900: loss = 0.13 (0.005 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 50888  Precision @ 1: 0.9252\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4663  Precision @ 1: 0.9326\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9259  Precision @ 1: 0.9259\n",
      "Step 5000: loss = 0.32 (0.004 sec)\n"
     ]
    }
   ],
   "source": [
    "library(tensorflow)\n",
    "\n",
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "NUM_CLASSES <- 10L\n",
    "\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE <- 28L\n",
    "IMAGE_PIXELS <- IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "# Basic model parameters as external flags.\n",
    "flags <- tf$app$flags\n",
    "flags$DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags$DEFINE_integer('max_steps', 5000L, 'Number of steps to run trainer.')\n",
    "flags$DEFINE_integer('hidden1', 128L, 'Number of units in hidden layer 1.')\n",
    "flags$DEFINE_integer('hidden2', 32L, 'Number of units in hidden layer 2.')\n",
    "flags$DEFINE_integer('batch_size', 100L, 'Batch size. Must divide evenly into the dataset sizes.')\n",
    "flags$DEFINE_string('train_dir', 'MNIST-data', 'Directory to put the training data.')\n",
    "flags$DEFINE_boolean('fake_data', FALSE, 'If true, uses fake data for unit testing.')\n",
    "FLAGS <- parse_flags()\n",
    "\n",
    "# input_data\n",
    "input_data <- tf$contrib$learn$datasets$mnist\n",
    "\n",
    "\n",
    "# Functions to builds the MNIST network.\n",
    "#\n",
    "# Implements the inference/loss/training pattern for model building.\n",
    "#\n",
    "# 1. inference() - Builds the model as far as is required for running the network\n",
    "# forward to make predictions.\n",
    "# 2. loss() - Adds to the inference model the layers required to generate loss.\n",
    "# 3. training() - Adds to the loss model the Ops required to generate and\n",
    "# apply gradients.\n",
    "\n",
    "# Build the MNIST model up to where it may be used for inference.\n",
    "#\n",
    "# Args:\n",
    "#   images: Images placeholder, from inputs().\n",
    "#   hidden1_units: Size of the first hidden layer.\n",
    "#   hidden2_units: Size of the second hidden layer.\n",
    "#\n",
    "# Returns:\n",
    "#   softmax_linear: Output tensor with the computed logits.\n",
    "#\n",
    "inference <- function(images, hidden1_units, hidden2_units) {\n",
    "  \n",
    "  # Hidden 1\n",
    "  with(tf$name_scope('hidden1'), {\n",
    "    weights <- tf$Variable(\n",
    "      tf$truncated_normal(shape(IMAGE_PIXELS, hidden1_units),\n",
    "                          stddev = 1.0 / sqrt(IMAGE_PIXELS)),\n",
    "      name = 'weights'\n",
    "    )\n",
    "    biases <- tf$Variable(tf$zeros(shape(hidden1_units),\n",
    "                                   name = 'biases'))\n",
    "    hidden1 <- tf$nn$relu(tf$matmul(images, weights) + biases)\n",
    "  })\n",
    "  \n",
    "  # Hidden 2\n",
    "  with(tf$name_scope('hidden2'), {\n",
    "    weights <- tf$Variable(\n",
    "      tf$truncated_normal(shape(hidden1_units, hidden2_units),\n",
    "                          stddev = 1.0 / sqrt(hidden1_units)),\n",
    "      name = 'weights')\n",
    "    biases <- tf$Variable(tf$zeros(shape(hidden2_units)),\n",
    "                          name = 'biases')\n",
    "    hidden2 <- tf$nn$relu(tf$matmul(hidden1, weights) + biases)\n",
    "  })\n",
    "  \n",
    "  # Linear\n",
    "  with(tf$name_scope('softmax_linear'), {\n",
    "    weights <- tf$Variable(\n",
    "      tf$truncated_normal(shape(hidden2_units, NUM_CLASSES),\n",
    "                          stddev = 1.0 / sqrt(hidden2_units)),\n",
    "      name = 'weights')\n",
    "    biases <- tf$Variable(tf$zeros(shape(NUM_CLASSES)),\n",
    "                          name = 'biases')\n",
    "    logits <- tf$matmul(hidden2, weights) + biases\n",
    "  })\n",
    "  \n",
    "  # return logits\n",
    "  logits\n",
    "}\n",
    "\n",
    "# Calculates the loss from the logits and the labels.\n",
    "#\n",
    "# Args:\n",
    "#   logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "#   labels: Labels tensor, int32 - [batch_size].\n",
    "#\n",
    "# Returns:\n",
    "#   loss: Loss tensor of type float.\n",
    "#\n",
    "loss <- function(logits, labels) {\n",
    "  labels <- tf$to_int64(labels)\n",
    "  cross_entropy <- tf$nn$sparse_softmax_cross_entropy_with_logits(\n",
    "    logits = logits, labels = labels, name = 'xentropy')\n",
    "  tf$reduce_mean(cross_entropy, name = 'xentropy_mean')\n",
    "}\n",
    "\n",
    "# Sets up the training Ops.\n",
    "#\n",
    "# Creates a summarizer to track the loss over time in TensorBoard.\n",
    "#\n",
    "# Creates an optimizer and applies the gradients to all trainable variables.\n",
    "#\n",
    "# The Op returned by this function is what must be passed to the\n",
    "# `sess.run()` call to cause the model to train.\n",
    "#\n",
    "# Args:\n",
    "#   loss: Loss tensor, from loss().\n",
    "#   learning_rate: The learning rate to use for gradient descent.\n",
    "#\n",
    "# Returns:\n",
    "#   train_op: The Op for training.\n",
    "#\n",
    "training <- function(loss, learning_rate) {\n",
    "  \n",
    "  # Add a scalar summary for the snapshot loss.\n",
    "  tf$summary$scalar(loss$op$name, loss)\n",
    "  \n",
    "  # Create the gradient descent optimizer with the given learning rate.\n",
    "  optimizer <- tf$train$GradientDescentOptimizer(learning_rate)\n",
    "  \n",
    "  # Create a variable to track the global step.\n",
    "  global_step <- tf$Variable(0L, name = 'global_step', trainable = FALSE)\n",
    "  \n",
    "  # Use the optimizer to apply the gradients that minimize the loss\n",
    "  # (and also increment the global step counter) as a single training step.\n",
    "  optimizer$minimize(loss, global_step = global_step)\n",
    "}\n",
    "\n",
    "# Evaluate the quality of the logits at predicting the label.\n",
    "#\n",
    "# Args:\n",
    "#   logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "#   labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "#           range [0, NUM_CLASSES).\n",
    "#\n",
    "# Returns:\n",
    "#   A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "#   that were predicted correctly.\n",
    "evaluation <- function(logits, labels) {\n",
    "  \n",
    "  # For a classifier model, we can use the in_top_k Op.\n",
    "  # It returns a bool tensor with shape [batch_size] that is true for\n",
    "  # the examples where the label is in the top k (here k=1)\n",
    "  # of all logits for that example.\n",
    "  correct <- tf$nn$in_top_k(logits, labels, 1L)\n",
    "  tf$reduce_sum(tf$cast(correct, tf$int32))\n",
    "}\n",
    "\n",
    "# Generate placeholder variables to represent the input tensors.\n",
    "#\n",
    "# These placeholders are used as inputs by the rest of the model building\n",
    "# code and will be fed from the downloaded data in the .run() loop, below.\n",
    "#\n",
    "# Args:\n",
    "#   batch_size: The batch size will be baked into both placeholders.\n",
    "#\n",
    "# Returns:\n",
    "#   placeholders$images: Images placeholder.\n",
    "#   placeholders$labels: Labels placeholder.\n",
    "#\n",
    "placeholder_inputs <- function(batch_size) {\n",
    "\n",
    "  # Note that the shapes of the placeholders match the shapes of the full\n",
    "  # image and label tensors, except the first dimension is now batch_size\n",
    "  # rather than the full size of the train or test data sets.\n",
    "  images <- tf$placeholder(tf$float32, shape(batch_size, IMAGE_PIXELS))\n",
    "  labels <- tf$placeholder(tf$int32, shape(batch_size))\n",
    "\n",
    "  # return both placeholders\n",
    "  list(images = images, labels = labels)\n",
    "}\n",
    "\n",
    "# Fills the feed_dict for training the given step.\n",
    "#\n",
    "# A feed_dict takes the form of:\n",
    "#   feed_dict = dict(\n",
    "#     <placeholder = <tensor of values to be passed for placeholder>,\n",
    "#     ....\n",
    "#   )\n",
    "#\n",
    "# Args:\n",
    "#   data_set: The set of images and labels, from input_data.read_data_sets()\n",
    "#   images_pl: The images placeholder, from placeholder_inputs().\n",
    "#   labels_pl: The labels placeholder, from placeholder_inputs().\n",
    "#\n",
    "# Returns:\n",
    "#   feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "#\n",
    "fill_feed_dict <- function(data_set, images_pl, labels_pl) {\n",
    "  # Create the feed_dict for the placeholders filled with the next\n",
    "  # `batch size` examples.\n",
    "  batch <- data_set$next_batch(FLAGS$batch_size, FLAGS$fake_data)\n",
    "  images_feed <- batch[[1]]\n",
    "  labels_feed <- batch[[2]]\n",
    "  dict(\n",
    "    images_pl = images_feed,\n",
    "    labels_pl = labels_feed\n",
    "  )\n",
    "}\n",
    "\n",
    "# Runs one evaluation against the full epoch of data.\n",
    "#\n",
    "# Args:\n",
    "#   sess: The session in which the model has been trained.\n",
    "#   eval_correct: The Tensor that returns the number of correct predictions.\n",
    "#   images_placeholder: The images placeholder.\n",
    "#   labels_placeholder: The labels placeholder.\n",
    "#   data_set: The set of images and labels to evaluate,\n",
    "#             from input_data.read_data_sets().\n",
    "#\n",
    "do_eval <- function(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_set) {\n",
    "  # And run one epoch of eval.\n",
    "  true_count <- 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch <- data_set$num_examples %/% FLAGS$batch_size\n",
    "  num_examples <- steps_per_epoch * FLAGS$batch_size\n",
    "  for (step in 1:steps_per_epoch) {\n",
    "    feed_dict <- fill_feed_dict(data_set,\n",
    "                                images_placeholder,\n",
    "                                labels_placeholder)\n",
    "    true_count <- true_count + sess$run(eval_correct, feed_dict=feed_dict)\n",
    "  }\n",
    "\n",
    "  precision <- true_count / num_examples\n",
    "  cat(sprintf('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f\\n',\n",
    "              num_examples, true_count, precision))\n",
    "}\n",
    "\n",
    "\n",
    "# Train MNIST for a number of steps.\n",
    "\n",
    "# Get the sets of images and labels for training, validation, and\n",
    "# test on MNIST.\n",
    "data_sets <- input_data$read_data_sets(FLAGS$train_dir, FLAGS$fake_data)\n",
    "\n",
    "# Tell TensorFlow that the model will be built into the default Graph.\n",
    "with(tf$Graph()$as_default(), {\n",
    "\n",
    "  # Generate placeholders for the images and labels.\n",
    "  placeholders <- placeholder_inputs(FLAGS$batch_size)\n",
    "\n",
    "  # Build a Graph that computes predictions from the inference model.\n",
    "  logits <- inference(placeholders$images, FLAGS$hidden1, FLAGS$hidden2)\n",
    "\n",
    "  # Add to the Graph the Ops for loss calculation.\n",
    "  loss <- loss(logits, placeholders$labels)\n",
    "\n",
    "  # Add to the Graph the Ops that calculate and apply gradients.\n",
    "  train_op <- training(loss, FLAGS$learning_rate)\n",
    "\n",
    "  # Add the Op to compare the logits to the labels during evaluation.\n",
    "  eval_correct <- evaluation(logits, placeholders$labels)\n",
    "\n",
    "  # Build the summary Tensor based on the TF collection of Summaries.\n",
    "  summary <- tf$summary$merge_all()\n",
    "\n",
    "  # Add the variable initializer Op.\n",
    "  init <- tf$global_variables_initializer()\n",
    "\n",
    "  # Create a saver for writing training checkpoints.\n",
    "  saver <- tf$train$Saver()\n",
    "\n",
    "  # Create a session for running Ops on the Graph.\n",
    "  sess <- tf$Session()\n",
    "\n",
    "  # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "  summary_writer <- tf$summary$FileWriter(FLAGS$train_dir, sess$graph)\n",
    "\n",
    "  # And then after everything is built:\n",
    "\n",
    "  # Run the Op to initialize the variables.\n",
    "  sess$run(init)\n",
    "\n",
    "  # Start the training loop.\n",
    "  for (step in 1:FLAGS$max_steps) {\n",
    "\n",
    "    start_time <- Sys.time()\n",
    "\n",
    "    # Fill a feed dictionary with the actual set of images and labels\n",
    "    # for this particular training step.\n",
    "    feed_dict <- fill_feed_dict(data_sets$train,\n",
    "                                placeholders$images,\n",
    "                                placeholders$labels)\n",
    "\n",
    "    # Run one step of the model.  The return values are the activations\n",
    "    # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "    # inspect the values of your Ops or variables, you may include them\n",
    "    # in the list passed to sess.run() and the value tensors will be\n",
    "    # returned in the tuple from the call.\n",
    "    values <- sess$run(list(train_op, loss), feed_dict = feed_dict)\n",
    "    loss_value <- values[[2]]\n",
    "\n",
    "    duration <- Sys.time() - start_time\n",
    "\n",
    "    # Write the summaries and print an overview fairly often.\n",
    "    if (step %% 100 == 0) {\n",
    "      # Print status to stdout.\n",
    "      cat(sprintf('Step %d: loss = %.2f (%.3f sec)\\n',\n",
    "                  step, loss_value, duration))\n",
    "      # Update the events file.\n",
    "      summary_str <- sess$run(summary, feed_dict=feed_dict)\n",
    "      summary_writer$add_summary(summary_str, step)\n",
    "      summary_writer$flush()\n",
    "    }\n",
    "\n",
    "    # Save a checkpoint and evaluate the model periodically.\n",
    "    if ((step + 1) %% 1000 == 0 || (step + 1) == FLAGS$max_steps) {\n",
    "      checkpoint_file <- file.path(FLAGS$train_dir, 'checkpoint')\n",
    "      saver$save(sess, checkpoint_file, global_step=step)\n",
    "      # Evaluate against the training set.\n",
    "      cat('Training Data Eval:\\n')\n",
    "      do_eval(sess,\n",
    "              eval_correct,\n",
    "              placeholders$images,\n",
    "              placeholders$labels,\n",
    "              data_sets$train)\n",
    "      # Evaluate against the validation set.\n",
    "      cat('Validation Data Eval:\\n')\n",
    "      do_eval(sess,\n",
    "              eval_correct,\n",
    "              placeholders$images,\n",
    "              placeholders$labels,\n",
    "              data_sets$validation)\n",
    "      # Evaluate against the test set.\n",
    "      cat('Test Data Eval:\\n')\n",
    "      do_eval(sess,\n",
    "              eval_correct,\n",
    "              placeholders$images,\n",
    "              placeholders$labels,\n",
    "              data_sets$test)\n",
    "    }\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
