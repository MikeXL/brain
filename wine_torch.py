# -*- coding: utf-8 -*-
"""wine_torch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/MikeXL/brain/blob/master/wine_torch.ipynb

# MJ LOG 0130.031118

Finnaly it meet the need for a vanilla neural net to predict the wine class based on the available features.

**Data prep**
*sklearn* does provide preprocessing and train, validation, test split

**Data exploration**
*matplot* is convenient enough for simple histogram, boxplot, trend lines, bar chart, scatter plot, 
[gallery](https://matplotlib.org/2.1.1/gallery/index.html) here to have fun

**Modeling**
*pytorch* is intuitive enough to use, with minor painful points that have to solve along the way such as 
weight reset in order to start clean, that has not been mentioned in documentation, or at least I have not seen 
rather than in forum post.Other than that it is a smooth ride.

*Hyperarameter tuning* such as learning rate along with different optimization technique (L-BFGS, Adam, SGD, RMSprop etc.) 
and plot their performance against one another. Perhaps throw in different loss function too. Did the _lr hunt_


**Validation**
Currently simply calculating the misclassification rate, not ideal. 
Validate model with validation dataset, along with measures like ROC, lift, gain, confusion matrix, F1, refer [here](http://scikit-learn.org/stable/modules/model_evaluation.html) from sklearn. _Lift_ and _gain_ would've been easier sell to business community.


Next step for me, 
Move onto attention network, autoencoder, Q network and GAN. Watch for my [brain repo](https://github.com/MikeXL/brain) for update.
"""

# !pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu90/torch_nightly.html

import torch
import torch.nn as nn

import numpy as np
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt

import sklearn
from sklearn import datasets
wine = datasets.load_wine()

##### exploration 
plt.plot(wine.data)
wine.feature_names

plt.scatter(wine.data[:, -4], wine.data[:, -3])

plt.boxplot(wine.data[:, 0])

print wine.DESCR

#### prep ... nornalization, just simple standardization or scale
wine.data = sklearn.preprocessing.scale(wine.data)   ## scale for prior feeding into nnet

plt.plot(wine.data)

x = torch.from_numpy(wine.data).float()       ## features
y = torch.from_numpy(wine.target).long()      ## target

#### vanilla neural net (mlp)
model = nn.Sequential(
  nn.Linear(13,24),
  nn.ReLU(),
  nn.Linear(24,12),
  nn.ReLU(),
  nn.Linear(12,3),
  nn.Softmax(1)
)

## reset the nn weight to start clean, and retrain
def weight_reset(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
        m.reset_parameters()

## simple scoring technique .. misclassification 
def score(model, x, y):
  model.eval()
  out=model(x)
  _, pred = torch.max(out, 1)
  print "Misclassified %s/%s" % ((y.size(0) - (pred.data==y.data).sum(0)), y.size(0))


EPOCH = 500
lr = .1
lf = nn.CrossEntropyLoss()

# train L-BFGS
def fit_lbfgs(model, optim, Floss, epoch):
  losses = []
  for _ in range(epoch):
    def closure():
      optim.zero_grad()
      yhat = model(x)
      loss = Floss(yhat, y)
      losses.append(loss.item())
      loss.backward()
      return loss
    optim.step(closure)
    ## might need this code at somepoint to plug in for validation
    ## with torch.no_grad():
    ##  pred = model(x))
    ##  l = lf(pred, y)
  return losses


# train Adam, SGD, RMSprop
def fit(model, optim, Floss, epoch):
  losses = []
  for _ in range(epoch):
    optim.zero_grad()
    yhat = model(x)
    loss = Floss(yhat, y)
    losses.append(loss.item())
    loss.backward()
    optim.step()
    ## might need this code at somepoint to plug in for validation
    ## with torch.no_grad():
    ##  pred = model(x))
    ##  l = lf(pred, y)
  return losses


## a little experiment on different learning rate and optimizers
##    learning rate 1 0.1 0.01 0.001
##    optimizers: L-BFGS, Adam, SGD, RMSprop
##
for lr in [1, .1, .01, .001]:
  optim_lbfgs   = torch.optim.LBFGS(model.parameters(), lr=lr)
  optim_adam    = torch.optim.Adam(model.parameters(), lr=lr)
  optim_sgd     = torch.optim.SGD(model.parameters(), lr=lr)
  optim_rmsprop = torch.optim.RMSprop(model.parameters(), lr=lr)

  idx = int(np.log10(lr*1000))

  model.apply(weight_reset)
  model.train()
  loss_lbfgs[idx] = fit_lbfgs(model, optim_lbfgs, lf, EPOCH)
  score(model, x, y)

  model.apply(weight_reset)
  model.train()
  loss_adam[idx] = fit(model, optim_adam, lf, EPOCH)
  score(model, x, y)

  model.apply(weight_reset)
  model.train()
  loss_sgd[idx] = fit(model, optim_sgd, lf, EPOCH)
  score(model, x, y)

  model.apply(weight_reset)
  model.train()
  loss_rmsprop[idx] = fit(model, optim_rmsprop, lf, EPOCH)
  score(model, x, y)

##
## not fancy, glimpse into the losses
##
plt.plot(loss_lbfgs[3], 'b-', label="L-BFGS lr=1")
plt.plot(loss_adam[3], 'b--', label="Adam lr=1")
plt.plot(loss_sgd[3], 'b-.', label="SGD lr=1")
plt.plot(loss_rmsprop[3], 'b:*', label="RMSprop lr=1")
plt.plot(loss_lbfgs[2], 'g-', label="L-BFGS lr=.1")
plt.plot(loss_adam[2], 'g--', label="Adam lr=.1")
plt.plot(loss_sgd[2], 'g-.', label="SGD lr=.1")
plt.plot(loss_rmsprop[2], 'g:*', label="RMSprop lr=.1")
plt.plot(loss_lbfgs[1], 'r-', label="L-BFGS lr=.01")
plt.plot(loss_adam[1], 'r--', label="Adam lr=.01")
plt.plot(loss_sgd[1], 'r-.', label="SGD lr=.01")
plt.plot(loss_rmsprop[1], 'r:*', label="RMSprop lr=.01")
plt.plot(loss_lbfgs[0], 'k-', label="L-BFGS lr=.001")
plt.plot(loss_adam[0], 'k--', label="Adam lr=.001")
plt.plot(loss_sgd[0], 'k-.', label="SGD lr=.001")
plt.plot(loss_rmsprop[0], 'k:*', label="RMSprop lr=.001")
plt.legend(loc="lower right")
plt.title("all in all")
plt.style.use('fivethirtyeight')

fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)
plt.style.use('fivethirtyeight')

axs[0,0].plot(loss_lbfgs[3], 'b-', label="L-BFGS lr=1")
axs[0,0].plot(loss_lbfgs[2], 'g-', label="L-BFGS lr=.1")
axs[0,0].plot(loss_lbfgs[1], 'r-', label="L-BFGS lr=.01")
axs[0,0].plot(loss_lbfgs[0], 'k-', label="L-BFGS lr=.001")
axs[0,0].legend(loc="lower right")
#axs[0,0].text("L-BFGS")



axs[1,0].plot(loss_adam[3], 'b--', label="Adam lr=1")
axs[1,0].plot(loss_adam[2], 'g--', label="Adam lr=.1")
axs[1,0].plot(loss_adam[1], 'r--', label="Adam lr=.01")
axs[1,0].plot(loss_adam[0], 'k--', label="Adam lr=.001")
axs[1,0].legend(loc="lower right")
#axs[0,1].title("Adam")

axs[0,1].plot(loss_sgd[3], 'b-.', label="SGD lr=1")
axs[0,1].plot(loss_sgd[2], 'g-.', label="SGD lr=.1")
axs[0,1].plot(loss_sgd[1], 'r-.', label="SGD lr=.01")
axs[0,1].plot(loss_sgd[0], 'k-.', label="SGD lr=.001")
axs[0,1].legend(loc="lower right")
#axs[1,0].title("SGD")

axs[1,1].plot(loss_rmsprop[3], 'b:*', label="RMSprop lr=1")
axs[1,1].plot(loss_rmsprop[2], 'g:*', label="RMSprop lr=.1")
axs[1,1].plot(loss_rmsprop[1], 'r:*', label="RMSprop lr=.01")
axs[1,1].plot(loss_rmsprop[0], 'k:*', label="RMSprop lr=.001")
axs[1,1].legend(loc="lower right")
#axs[1,1].title("RMSprop")


#plt.tight_layout()
#plt.show()

#plt.plot(loss_lbfgs[3], 'b-', label="L-BFGS lr=1")
#plt.plot(loss_adam[3], 'b--', label="Adam lr=1")
plt.plot(loss_sgd[3], 'b-.', label="SGD lr=1")
#plt.plot(loss_rmsprop[3], 'b:*', label="RMSprop lr=1")
plt.plot(loss_lbfgs[2], 'g-', label="L-BFGS lr=.1")
plt.plot(loss_adam[2], 'g--', label="Adam lr=.1")
plt.plot(loss_sgd[2], 'g-.', label="SGD lr=.1")
#plt.plot(loss_rmsprop[2], 'g:*', label="RMSprop lr=.1")
#plt.plot(loss_lbfgs[1], 'r-', label="L-BFGS lr=.01")
plt.plot(loss_adam[1], 'r--', label="Adam lr=.01")
#plt.plot(loss_sgd[1], 'r-.', label="SGD lr=.01")
plt.plot(loss_rmsprop[1], 'r:*', label="RMSprop lr=.01")
plt.plot(loss_lbfgs[0], 'k-', label="L-BFGS lr=.001")
plt.plot(loss_adam[0], 'k--', label="Adam lr=.001")
#plt.plot(loss_sgd[0], 'k-.', label="SGD lr=.001")
plt.plot(loss_rmsprop[0], 'k:*', label="RMSprop lr=.001")
plt.legend(loc="lower right")
plt.title("chick chick winner diner")
plt.style.use('fivethirtyeight')

print optim_lbfgs.state_dict
print optim_adam.state_dict
print optim_sgd.state_dict
print optim_rmsprop.state_dict

print (optim_lbfgs.param_groups[0].items())[4]
print (optim_adam.param_groups[0].items())[4]
print (optim_sgd.param_groups[0].items())[4]
print (optim_rmsprop.param_groups[0].items())[4]

