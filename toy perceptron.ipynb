{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                    #\n",
    "                                                                    #\n",
    "                                                                    # MJ LOG: 1419.121117\n",
    "                                                                    #\n",
    "                                                                    # finally, phew, \n",
    "                                                                    # nearly completed this journey to a single perceptron\n",
    "                                                                    # and learning from deeplearning.ai\n",
    "                                                                    # a lot math and calculus, but even more fun to write it in R\n",
    "                                                                    #\n",
    "                                                                    # the idea of nn is quite simple and straightforward\n",
    "                                                                    # taking the input, excite neurons and produce adenosine\n",
    "                                                                    # then it comes down to reduce noises (optimization) \n",
    "                                                                    # and the speed (optimization again for network converge)\n",
    "                                                                    # most of the business problems does not require CNN or RNN\n",
    "                                                                    # rather solved by single or double hidden layer of\n",
    "                                                                    # vanilla neural networks\n",
    "                                                                    #\n",
    "                                                                    # MJ LOG: 2200.121117\n",
    "                                                                    #\n",
    "                                                                    # for choosing number of layers\n",
    "                                                                    #     One hidden layer is sufficient for large majority of problems.\n",
    "                                                                    #     It is like huh?!\n",
    "                                                                    #\n",
    "                                                                    # 0 - Only capable of representing \n",
    "                                                                    #     linear separable functions or decisions.\n",
    "                                                                    # 1 - Can approximate any function that contains a continuous mapping\n",
    "                                                                    #     from one finite space to another.\n",
    "                                                                    # 2 - Can represent an arbitrary decision boundary to \n",
    "                                                                    #     arbitrary accuracy with rational activation functions and \n",
    "                                                                    #     can approximate any smooth mapping to any accuracy.\n",
    "                                                                    #\n",
    "                                                                    # for choosing number of neurons\n",
    "                                                                    #     somewhere between number of inputs and output\n",
    "                                                                    #     I tend to like the square root formula below\n",
    "                                                                    #\n",
    "                                                                    #     n = n.obs / (2 * (n.parm + n.target))\n",
    "                                                                    #                 the scaling factore can be 2~\n",
    "                                                                    #     n = sqrt (n.parm * n.target)\n",
    "                                                                    #\n",
    "                                                                    #    the above formula are not golden rules, rather reference\n",
    "                                                                    #  \"Very simple. \n",
    "                                                                    #   Just keep adding layers until the test error does not improve anymore.\"\n",
    "                                                                    #  - G. Hinton\n",
    "                                                                    #\n",
    "                                                                    # again, some calls parameters, some calls input, \n",
    "                                                                    # while others call features; patato (perteito, pertahto)\n",
    "                                                                    #\n",
    "                                                                    # MJ LOG: 0112.131117\n",
    "                                                                    #\n",
    "                                                                    # set two parameters for iterations and bias\n",
    "                                                                    #\n",
    "                                                                    #\n",
    "make.it.so <- function(iter=100, bias=0){                           #\n",
    "                                                                    #\n",
    "                                                                    # Prepare data for model \n",
    "                                                                    #            species ~ Sepal.Length + Sepal.Width\n",
    "                                                                    #\n",
    "  x1 <- head(iris$Sepal.Length, 100)                                # iris sepal length\n",
    "  x2 <- head(iris$Sepal.Width, 100)                                 # iris sepal width\n",
    "  x  <- matrix(cbind(x1, x2), nrow=100, ncol = 2)                   # \n",
    "  y  <- append(rep(1, 50), rep(-1, 50))                             # setosa = 1, versicolor = -1, discard virginica\n",
    "                                                                    # until one day I'm drunk enough to write softmax function\n",
    "                                                                    #\n",
    "                                                                    # vannila it is\n",
    "                                                                    #        2 inputs\n",
    "                                                                    #        3 layers (input, hidden, output)\n",
    "                                                                    #        1 perceptron\n",
    "                                                                    #        1 output with 2 levels\n",
    "                                                                    #\n",
    "  n.iter <- iter                                                    # number of iterations\n",
    "  epoch  <- 1                                                       # mini batch, no used here\n",
    "                                                                    #   epoch recommendation 2^(6~9) = 64, 128, 256, 512\n",
    "                                                                    #   RoT: can fit into CPU/GPU memory\n",
    "  n.obs  <- nrow(x)                                                 # number of samples / observations\n",
    "  n.parm <- ncol(x)                                                 # number of parameters / input / features\n",
    "  b      <- bias                                                    # bias, not really used here, or could try out .01\n",
    "                                                                    #       mind bias is per neuron, \n",
    "                                                                    #       and can be initialized the same way as weights\n",
    "  alpha  <- 0.1                                                     # learning rate\n",
    "  y.hat  <- rep(0, n.obs)                                           # predications y.hat, baseline\n",
    "  prob   <- rep(0, n.obs)                                           #   probability\n",
    "  loss   <- rep(0, n.iter)                                          #   record log loss for each iteration\n",
    "  w      <- rnorm(2) * .01                                          # initialize weight, start small\n",
    "  # w    <- w/sum(w^2)                                              # normalize the initialization, overkill for this ?\n",
    "                                                                    # large w would end up at the flat side of tanh\n",
    "                                                                    # slow down the gradient descent, slow to converge for whole nn\n",
    "  eps    <- 1e-15                                                   # epsilon, threshold for log loss to deal corner case of 0 and 1  \n",
    "                                                                    #\n",
    "  for(i in 1:n.iter) {                                              # iterate\n",
    "    for(j in 1:n.obs){                                              #  through all observations / samples\n",
    "      p        <- tanh(w[1]*x[j, 1] + w[2]*x[j, 2] + b)             # tanh activation\n",
    "                                                                    # other activation function like \n",
    "                                                                    #     cos\n",
    "                                                                    #     sigmoid/logistic\n",
    "                                                                    #     softmax for multi-class target\n",
    "                                                                    #     linear, without applying any function\n",
    "                                                                    #     softplus, ReLU\n",
    "                                                                    #\n",
    "      prob[j]  <- ifelse(p>0, p, 1+p)                               # probablity for y.hat==1\n",
    "      prob[j]  <- pmin(pmax(prob[j], 1e-15), 1-1e-15)               # account 0 and 1 value for log loss\n",
    "      y.hat[j] <- sign(p)                                           # predicated y\n",
    "                                                                    # perhaps better use to account 0\n",
    "                                                                    # doubting the function of sign, as it would miss the value 0\n",
    "                                                                    # perhaps better be explicit on >=0 than using sign\n",
    "                                                                    # y.hat    <- (p>=0)\n",
    "      eta  <- y[j] - y.hat[j]                                       # error\n",
    "                                                                    # if this is kept, then MAE, MSE can be calculated later\n",
    "                                                                    # see previous commits, MAE, MSE plots were removed\n",
    "                                                                    # loss     <- -(y * log(p) + (1 - y) * log(1 - p))\n",
    "                                                                    # as they really not a good indicator\n",
    "      w[1] <- w[1] + alpha * eta * x[j,1]                           # update w1, stochastic gradient descent\n",
    "      w[2] <- w[2] + alpha * eta * x[j,2]                           # update w2\n",
    "      b    <- b    + alpha * eta * b                                # update bias\n",
    "      # alpha = alpha /(1+decay_rate * epoch)                       # learning rate decay, might be last thing to try for tuning\n",
    "                                                                    #    alpha = power(.95, epoch) * alpha\n",
    "                                                                    #    alpha = k / sqrt(epoch)   * alpha\n",
    "                                                                    #    manual decay\n",
    "                                                                    # feels like monte carlo simulation all over again, um\n",
    "                                                                    #\n",
    "    }                                                               #  // END LOOP OBSERVATIONS //\n",
    "                                                                    # log loss L(...) = -(y*log(p) + (1-y)*log(1-p))\n",
    "                                                                    # kaggle usually uses log loss to score classifier performance\n",
    "                                                                    # might be a bad naming convention here\n",
    "                                                                    # some say loss and cost are synonymous\n",
    "                                                                    # some say loss is for observation (sample) computation,    \n",
    "                                                                    #          cost is for full data set\n",
    "                                                                    #     objective is for gernal naming of the objective function\n",
    "                                                                    # I'm like whatever, just pick the one you like\n",
    "    loss[i] <- -(mean(y * log(prob) + (1 - y) * log(1 - prob)))     #\n",
    "                                                                    # derivative log loss seems more appropriate for nn\n",
    "                                                                    #          dL = -(y/y.hat) + (1-y)/(1-y.hat)\n",
    "                                                                    # dL[i] <- -mean(y/prob+(1-y)/(1-prob))                   \n",
    "                                                                    # early stop to avoid overfitting\n",
    "                                                                    # but when ?\n",
    "                                                                    #  monitoring the converge trend of w ?\n",
    "                                                                    #  monitoring log loss or derivate log loss\n",
    "                                                                    #  cross validation to compare training error and validation error ?\n",
    "                                                                    #\n",
    "                                                                    # batch norm ?\n",
    "                                                                    # reduce cov shift through layers\n",
    "                                                                    #\n",
    "                                                                    # test time considerations of mu, sigma for normalization\n",
    "                                                                    # estimate using exponentially weighted average\n",
    "                                                                    # across mini batches\n",
    "                                                                    #\n",
    "  }                                                                 # // END LOOP ITER //\n",
    "                                                                    #\n",
    "  cat(\"Results for last iteration:\", i, \"\\n\")                       # show 'n tell\n",
    "  cat(\"w1 = \", w[1], \"\\n\")                                          # weights of last iteration      \n",
    "  cat(\"w2 = \", w[2], \"\\n\")                                          #\n",
    "  cat(\"bias = \", b, \"\\n\")                                           # bias of last iteration\n",
    "  cat(\"Misclassification rate:\", sum(y.hat != y)/n.obs, \"\\n\")       # misclassification rate of last iteration\n",
    "  plot(table(y, y.hat), color=\"#66cc00\", main=\"confusion matrix\")   # confusion matrix of last iteration\n",
    "  plot(loss, xlab=\"# iteration\", ylab=\"log loss\", type=\"l\")         # log loss plot to see convergence \n",
    "                                                                    #\n",
    "                                                                    # how to store the model for future prediction ?\n",
    "                                                                    # pretty much doing the high school math for prediction \n",
    "                                                                    # remember to do the SIGN thing to get the classifier\n",
    "                                                                    #\n",
    "                                                                    #     pred = ACT(w1 * x1 + w2 * x2 + b)\n",
    "                                                                    #\n",
    "}                                                                   # // MAKE IT SO //\n",
    "                                                                    #\n",
    "                                                                    #\n",
    "                                                                    # dropouts in deep network\n",
    "                                                                    #  it seems insane to randomly kill neurons in the network\n",
    "                                                                    #  and shockingly it works\n",
    "                                                                    #  then think about that our brain cells die every single seconds\n",
    "                                                                    #  we turns out to be ok, well, suffering memory loss or hardly recall\n",
    "                                                                    #  the issue here is not to kill in training set, \n",
    "                                                                    #  rather how to treat validation and testing set, trick was ...\n",
    "                                                                    #  double the training weights (if my brain cell for that was not killed)\n",
    "                                                                    #  think again, it is like Darwin's evolution theory\n",
    "                                                                    #\n",
    "                                                                    # MJ LOG: 1818.141117\n",
    "                                                                    #\n",
    "                                                                    # ResNet\n",
    "                                                                    # CNN\n",
    "                                                                    # RNN\n",
    "                                                                    # Capsule\n",
    "                                                                    # Time series prediction\n",
    "                                                                    # Not really nn, but worth look into xgboost\n",
    "                                                                    #\n",
    "                                                                    # Advice from Hinton\n",
    "                                                                    #   If everyone tells you that is plain wrong, \n",
    "                                                                    #   then you know you are onto something.\n",
    "                                                                    #\n",
    "                                                                    # inception network\n",
    "                                                                    #\n",
    "                                                                    #\n",
    "                                                                    #\n",
    "                                                                    #\n",
    "                                                                    # LLAP\n",
    "                                                                    #\n",
    "                                                                    #     ___________________          _-_                         \n",
    "                                                                    #    \\==============_=_/ ____.---'---`---.____\n",
    "                                                                    #                \\_ \\    \\----._________.----/\n",
    "                                                                    #                  \\ \\   /  /    `-_-'\n",
    "                                                                    #              __,--`.`-'..'-_\n",
    "                                                                    #             /____          ||\n",
    "                                                                    #                  `--.____,-'\n",
    "                                                                    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for last iteration: 500 \n",
      "w1 =  -13.19677 \n",
      "w2 =  23.19687 \n",
      "bias =  0 \n",
      "Misclassification rate: 0.03 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1mzABoaGh8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\nQoIBQoIBQoIBQoIB/wdYvVmjRH1y1gAAAABJRU5ErkJggg==",
      "text/plain": [
       "Plot with title \"confusion matrix\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nRK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make.it.so(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
